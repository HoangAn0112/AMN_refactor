{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model using Scikit-learn and Keras wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- model ----------------------------------------\n",
      "number of metabolites:  1877\n",
      "filtered measurements size:  1\n",
      "dataset file: ./Dataset/biolog_iML1515_new_UB.npz\n",
      "model type: AMNWt\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (17400, 430) (17400, 1)\n",
      "training epochs: 50\n",
      "training regression: True\n",
      "training batch size: 7\n",
      "training validation iter: 0\n",
      "training early stopping: False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from aMNWtModel import AMNWtModel\n",
    "\n",
    "\n",
    "seed = 10\n",
    "# np.random.seed(seed=seed)  \n",
    "# tf.random.set_seed(seed)\n",
    "\n",
    "# dataset_file = \"./Dataset/IJN1463_EXP_UB_Anne.npz\"\n",
    "# objective=['BIOMASS_KT2440_WT3']\n",
    "\n",
    "# dataset_file = \"./Dataset/IJN1463_10_UB.npz\"\n",
    "# objective=['BIOMASS_KT2440_WT3']\n",
    "\n",
    "# dataset_file = \"./Dataset/e_coli_core_UB_100.npz\"\n",
    "# objective=['BIOMASS_Ecoli_core_w_GAM']\n",
    "# epoch = 200\n",
    "# batch_size = 7\n",
    "\n",
    "\n",
    "# dataset_file = \"./Dataset/biolog_iML1515_EXP_UB.npz\"\n",
    "# objective=['BIOMASS_Ec_iML1515_core_75p37M']\n",
    "# epoch = 20\n",
    "# batch_size = 30\n",
    "\n",
    "dataset_file = \"./Dataset/biolog_iML1515_new_UB.npz\"\n",
    "objective=['BIOMASS_Ec_iML1515_core_75p37M']\n",
    "epoch = 20\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "print(\"---------------------------------------- model ----------------------------------------\")\n",
    "model = AMNWtModel(dataset_file=dataset_file, \n",
    "                   objective=objective,\n",
    "                   timestep=4,\n",
    "                #    n_hidden=1, \n",
    "                   hidden_dim=50,\n",
    "                   epochs=50, \n",
    "                   verbose=True,\n",
    "                   batch_size=7)\n",
    "model.printout()\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler \n",
    "from tools import MaxScaler\n",
    "scaler= MaxScaler()\n",
    "model.train_test_split(test_size=0.1, random_state=seed)\n",
    "model.preprocess(scaler)\n",
    "model.preprocessing_for_specific_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:39:23.829147: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:23.830570: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 5. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-21 14:39:24.091534: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:24.092926: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 5. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-21 14:39:24.299801: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:24.301827: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 5. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-21 14:39:24.539480: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:24.540835: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 5. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-21 14:39:24.600563: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-09-21 14:39:24.805730: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:24.807419: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 5. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-09-21 14:39:24.876773: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-09-21 14:39:25.100381: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-09-21 14:39:25.355184: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-09-21 14:39:25.607271: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([4080.3157227 , 4080.54561496, 4076.42844009, 4087.82974768,\n",
       "        4065.37598562]),\n",
       " 'score_time': array([27.87980247, 29.91382384, 20.64488554, 27.45808148, 33.03862095]),\n",
       " 'test_loss_constraint': array([0.00536525, 0.01075396, 0.00571801, 0.00593194, 0.00590252]),\n",
       " 'train_loss_constraint': array([0.00546597, 0.01086891, 0.00557294, 0.00594837, 0.00586356]),\n",
       " 'test_mse': array([0.38555504, 0.52376219, 0.33764069, 0.34447866, 0.46628941]),\n",
       " 'train_mse': array([0.38361359, 0.5343125 , 0.33439099, 0.34913444, 0.46459096]),\n",
       " 'test_R2': array([-2.66452905, -3.9376391 , -2.25171746, -2.36596027, -3.54783449]),\n",
       " 'train_R2': array([-2.70585956, -4.14268876, -2.21163967, -2.34017798, -3.46586   ])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "estimator= KerasRegressor(build_fn=model.build_model, \n",
    "                          epochs=epoch, \n",
    "                          batch_size=batch_size, \n",
    "                          verbose=0)\n",
    "\n",
    "scoring = {\"loss_constraint\":make_scorer(model.loss_constraint),\n",
    "           \"mse\":make_scorer(model.mse),\n",
    "           \"R2\":make_scorer(model.R2),\n",
    "           }\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "fit_params = {'callbacks': [callback]}\n",
    "fit_params = {}\n",
    "\n",
    "kfold= KFold(n_splits=5,shuffle=True, random_state=seed)\n",
    "\n",
    "results=cross_validate(estimator, \n",
    "                       model.X_train, \n",
    "                       model.Y_train, \n",
    "                       cv=kfold, \n",
    "                       n_jobs=5, \n",
    "                       scoring=scoring, \n",
    "                       fit_params=fit_params,\n",
    "                       return_train_score=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 : -2.902497158588206\n",
      "Q2 : -3.2142680727890784\n"
     ]
    }
   ],
   "source": [
    "AMNWt_model = model.build_model()\n",
    "history = AMNWt_model.fit(model.X_train, model.Y_train, epochs=epoch, batch_size=batch_size, verbose=0)\n",
    "\n",
    "print(\"R2 :\", model.R2(model.Y_train, AMNWt_model.predict(model.X_train)))\n",
    "print(\"Q2 :\", model.R2(model.Y_test, AMNWt_model.predict(model.X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agiralt/anaconda3/envs/AMN/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc688cc4700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc6896b19d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "estimator= KerasRegressor(build_fn=model.build_model, \n",
    "                          epochs=epoch, \n",
    "                          batch_size=batch_size, \n",
    "                          verbose=0)\n",
    "\n",
    "distributions = dict(batch_size=[7,20],\n",
    "                     nb_epoch=[2,100],\n",
    "                    #  hidden_dim=[1,2],\n",
    "                     )\n",
    "\n",
    "scoring = {\"loss_constraint\":make_scorer(model.loss_constraint),\n",
    "           \"mse\":make_scorer(model.mse),\n",
    "           \"R2\":make_scorer(model.R2),\n",
    "           }\n",
    "\n",
    "clf = RandomizedSearchCV(estimator, distributions, random_state=0)\n",
    "search = clf.fit(model.X_test, model.Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- model ----------------------------------------\n",
      "number of metabolites:  72\n",
      "filtered measurements size:  1\n",
      "dataset file: ./Dataset/e_coli_core_UB_100.npz\n",
      "model type: AMNWt\n",
      "model medium bound: UB\n",
      "timestep: 4\n",
      "training set size (100, 20) (100, 1)\n",
      "training epochs: 50\n",
      "training regression: True\n",
      "training batch size: 7\n",
      "training validation iter: 0\n",
      "training early stopping: False\n",
      "R2 : 0.9512388023070849\n",
      "Q2 : 0.8899013443185595\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from aMNWtModel import AMNWtModel\n",
    "\n",
    "\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "dataset_file = \"./Dataset/e_coli_core_UB_100.npz\"\n",
    "objective=['BIOMASS_Ecoli_core_w_GAM']\n",
    "\n",
    "# Dataset plus model structure\n",
    "print(\"---------------------------------------- model ----------------------------------------\")\n",
    "model = AMNWtModel(dataset_file=dataset_file, \n",
    "                   objective=objective,\n",
    "                   timestep=4,\n",
    "                   hidden_dim=50,\n",
    "                   epochs=50, \n",
    "                   verbose=True,\n",
    "                   batch_size=7)\n",
    "model.printout()\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,MaxAbsScaler \n",
    "from tools import MaxScaler\n",
    "scaler= MaxScaler()\n",
    "model.train_test_split(test_size=0.1, random_state=seed)\n",
    "model.preprocess(scaler)\n",
    "model.preprocessing_for_specific_model()\n",
    "\n",
    "batch_size = 7\n",
    "\n",
    "# Construct and train an AMNWt model\n",
    "AMNWt_model = model.build_model()\n",
    "history = AMNWt_model.fit(model.X_train, model.Y_train, epochs=200, batch_size=batch_size, verbose=0)\n",
    "\n",
    "print(\"R2 :\", model.R2(model.Y_train, AMNWt_model.predict(model.X_train)))\n",
    "print(\"Q2 :\", model.R2(model.Y_test, AMNWt_model.predict(model.X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 : 0.9512388023070849\n",
      "Q2 : 0.8899013443185595\n"
     ]
    }
   ],
   "source": [
    "from aMNWtModel import RNNCell\n",
    "from tools import my_mse\n",
    "\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Recreate new model from config file, compile and train it. First test on config.\n",
    "config = AMNWt_model.get_config()\n",
    "AMNWt_model_= tf.keras.Model.from_config(config, custom_objects={\"RNNCell\":RNNCell})\n",
    "AMNWt_model_.compile(loss=my_mse,optimizer='adam',metrics=[my_mse])\n",
    "history = AMNWt_model_.fit(model.X_train, model.Y_train, epochs=200, batch_size=batch_size, verbose=0)\n",
    "\n",
    "print(\"R2 :\", model.R2(model.Y_train, AMNWt_model_.predict(model.X_train)))\n",
    "print(\"Q2 :\", model.R2(model.Y_test, AMNWt_model_.predict(model.X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train AMNWt_model\n",
    "model_file = \"Models/test_saving_model.keras\"\n",
    "tf.keras.models.save_model(AMNWt_model,model_file, overwrite=True, save_format=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 : 0.9819044696340576\n",
      "Q2 : 0.9144270593047028\n"
     ]
    }
   ],
   "source": [
    "from tools import my_mse\n",
    "from aMNWtModel import RNNCell\n",
    "\n",
    "AMNWt_model_ = tf.keras.models.load_model(model_file, custom_objects={\"RNNCell\":RNNCell,\n",
    "                                                                               \"my_mse\":my_mse})\n",
    "\n",
    "history = AMNWt_model_.fit(model.X_train, model.Y_train, epochs=200, batch_size=batch_size, verbose=0)\n",
    "print(\"R2 :\", model.R2(model.Y_train, AMNWt_model_.predict(model.X_train)))\n",
    "print(\"Q2 :\", model.R2(model.Y_test, AMNWt_model_.predict(model.X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
